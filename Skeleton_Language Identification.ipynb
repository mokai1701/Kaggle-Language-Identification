{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Language Identification\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"margin: 20px\">A beloved home South Africa is, with its melting pot of culture, art, tradition and boasts of 11 official languages found spread across all 9 of its Provinces. <br>\n",
    "The purpose of the notebook is to take a journey into the heart and richness of the 11 amazing languages it posseses by creating a classification model that can detect which  of the 11 languages a text could belong... </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of the classification approach followed is to  import the neccessary libraries that aid in:\n",
    "- Numerical computation\n",
    "- Data Import and manipulation\n",
    "- Data set split into training and test\n",
    "- Model training and prediction\n",
    "\n",
    "### Part 2 to follow is to import csv files namely of the:\n",
    "- Train set\n",
    "- Test set\n",
    "- Sample sumbission\n",
    "- Analyse the imported data\n",
    "\n",
    "### Part 3 of the notebook is for:\n",
    "- Training\n",
    "- Predictions\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Import helper libraries\n",
    "\n",
    "The libraries as mentioned are for numerical computation, data analysis and manipulation, training, predicting and scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Part 2: Import data and perform analysis\n",
    "\n",
    "This part of the notebook deals with reading in the data, analysing the data for the types of data types in the datasets, as this guides in the vectorization of features.\n",
    "The above is done because machine learning models cannot train on object type data, therefore during analysis, data types are accounted for.\n",
    "Missing values are also accounted for in the rows and also any whitespaces within the columns.\n",
    "Lastly, the label column classes are checked for data imbalancing to see the distribution of the classes and if any resampling will need to be performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Analysis of the train set\n",
    "\n",
    "The first analysis is performed on the train set in the the following cells below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "In data analysis, rows could or could not have missing values, in the case of missing values, pandas methods are used to handle such values in a desired manner. When dealing with text data however, extra caution needs to be taken, as some entries could not report as `NaN` in the summary statistic but could be saved as empty string or white spaces. The code below accounts for such a precaution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are well balanced with each other and resampling for this dataset will not be needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Analysis of the test set\n",
    "\n",
    "The second analysis is performed on the train set in the the following cells below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "As already mentioned above, in data analysis, rows could or could not have missing values, in the case of missing values, pandas methods are used to handle such values in a desired manner. When dealing with text data however, extra caution needs to be taken, as some entries could not report as `NaN` in the summary statistic but could be saved as empty string or white spaces. The code below accounts for such a precaution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Part 3: Training, Testing and Predictions\n",
    "\n",
    "- Training\n",
    "- Predictions\n",
    "___\n",
    "\n",
    "#### Training\n",
    "In this part of the notebook, the train set data is split into **`X`** features, using the 'text' column of the train dataframe and **`y`** labels using the 'lang_df' column of the train dataframe.\n",
    "The data is split as **`25% for the testing set and 75% for the training set with a random_state of 42`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation metric\n",
    "\n",
    "The evaluation metric of the is classification is the **`F1_score`**, depicted with the formula below:\n",
    "\n",
    "\n",
    "$$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$\n",
    "\n",
    "\n",
    "\n",
    "According to [deepai.org](https://deepai.org/machine-learning-glossary-and-terms/f-score).The F-score is a way of combining the precision and recall of the model, and it is defined as the harmonic mean of the modelâ€™s precision and recall. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Prediction on the test set\n",
    "\n",
    "A new dataframe is created that takes in the index from the test dataframe and adds a new column `lang_id` from the predictions of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lang_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>tsn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>nbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ssw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ssw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index lang_id\n",
       "0      1     tsn\n",
       "1      2     nbl\n",
       "2      3     ven\n",
       "3      4     ssw\n",
       "4      5     ssw"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first few entries to ensure it looks similar to the required sample_submission\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission file\n",
    "sub_df.to_csv('base_submission_logreg.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
